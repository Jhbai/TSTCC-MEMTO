{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a010146a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jhbai\\Anaconda\\envs\\MachineLearningEnv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Phase 1: TS-TCC Representation Learning ---\n",
      "Phase 1, Epoch 1/5, Loss: 2.6760\n",
      "Phase 1, Epoch 2/5, Loss: 2.3358\n",
      "Phase 1, Epoch 3/5, Loss: 2.0248\n",
      "Phase 1, Epoch 4/5, Loss: 1.7282\n",
      "Phase 1, Epoch 5/5, Loss: 1.4366\n",
      "\n",
      "--- Initializing Memory with K-means ---\n",
      "Memory initialized.\n",
      "\n",
      "--- Starting Phase 2: Hybrid MEMTO + TS-TCC Training ---\n",
      "Phase 2, Epoch 1/10, Total Loss: 2.3037 (MEMTO: 1.0440, TS-TCC: 1.2596)\n",
      "Phase 2, Epoch 2/10, Total Loss: 2.1522 (MEMTO: 1.0486, TS-TCC: 1.1036)\n",
      "Phase 2, Epoch 3/10, Total Loss: 2.0461 (MEMTO: 1.0268, TS-TCC: 1.0194)\n",
      "Phase 2, Epoch 4/10, Total Loss: 1.9688 (MEMTO: 1.0625, TS-TCC: 0.9063)\n",
      "Phase 2, Epoch 5/10, Total Loss: 1.9713 (MEMTO: 1.0422, TS-TCC: 0.9290)\n",
      "Phase 2, Epoch 6/10, Total Loss: 1.8558 (MEMTO: 1.0425, TS-TCC: 0.8133)\n",
      "Phase 2, Epoch 7/10, Total Loss: 1.8137 (MEMTO: 1.0444, TS-TCC: 0.7694)\n",
      "Phase 2, Epoch 8/10, Total Loss: 1.8523 (MEMTO: 1.0573, TS-TCC: 0.7950)\n",
      "Phase 2, Epoch 9/10, Total Loss: 1.7743 (MEMTO: 1.0372, TS-TCC: 0.7371)\n",
      "Phase 2, Epoch 10/10, Total Loss: 1.7730 (MEMTO: 1.0282, TS-TCC: 0.7448)\n",
      "\n",
      "Training and prediction finished.\n",
      "Shape of anomaly scores: (64, 100)\n",
      "Expected shape: (64, 100)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# --- 1. Encoder Module (from TS-TCC) ---\n",
    "class TSEncoder(nn.Module):\n",
    "    def __init__(self, input_dims, output_dims, hidden_dims=64, depth=3):\n",
    "        super(TSEncoder, self).__init__()\n",
    "        layers = []\n",
    "        current_in = input_dims\n",
    "        for _ in range(depth):\n",
    "            layers.extend([\n",
    "                nn.Conv1d(current_in, hidden_dims, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm1d(hidden_dims),\n",
    "                nn.ReLU()\n",
    "            ])\n",
    "            current_in = hidden_dims\n",
    "        layers.append(nn.Conv1d(hidden_dims, output_dims, kernel_size=1))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_len, input_dims]\n",
    "        x = x.permute(0, 2, 1)  # to [batch, input_dims, seq_len]\n",
    "        out = self.network(x)\n",
    "        return out.permute(0, 2, 1)  # to [batch, seq_len, output_dims]\n",
    "\n",
    "\n",
    "# --- 2. Hybrid MEMTO-TSTCC Model ---\n",
    "class MEMTO_TSTCC(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, num_memory_items, n_heads=4, n_layers=2):\n",
    "        super(MEMTO_TSTCC, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Encoder from TS-TCC\n",
    "        self.encoder = TSEncoder(input_dims=input_dim, output_dims=latent_dim)\n",
    "        \n",
    "        # --- Modules for TS-TCC Loss ---\n",
    "        tstcc_encoder_layers = nn.TransformerEncoderLayer(\n",
    "            d_model=latent_dim, nhead=n_heads, dim_feedforward=latent_dim * 2,\n",
    "            dropout=0.1, batch_first=True, norm_first=True\n",
    "        )\n",
    "        self.temporal_transformer = nn.TransformerEncoder(tstcc_encoder_layers, num_layers=n_layers)\n",
    "        self.temporal_predictor = nn.Linear(latent_dim, latent_dim)\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(latent_dim, latent_dim), nn.ReLU(), nn.Linear(latent_dim, latent_dim)\n",
    "        )\n",
    "\n",
    "        # --- Modules for MEMTO Logic ---\n",
    "        self.memory = nn.Parameter(torch.randn(num_memory_items, latent_dim), requires_grad=True)\n",
    "        self.U_psi = nn.Linear(latent_dim, latent_dim)\n",
    "        self.W_psi = nn.Linear(latent_dim, latent_dim)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim * 2, latent_dim), nn.ReLU(), nn.Linear(latent_dim, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_primary, x_secondary=None, phase='phase2', future_k=5, temperature=0.1):\n",
    "        if x_secondary is None:\n",
    "            x_secondary = x_primary # For prediction or single-view operations\n",
    "\n",
    "        # --- Feature Extraction ---\n",
    "        q_primary = self.encoder(x_primary)\n",
    "        q_secondary = self.encoder(x_secondary)\n",
    "\n",
    "        # --- TS-TCC Path (for loss calculation in both phases) ---\n",
    "        c_primary = self.temporal_transformer(q_primary[:, :-future_k, :]).mean(dim=1)\n",
    "        c_secondary = self.temporal_transformer(q_secondary[:, :-future_k, :]).mean(dim=1)\n",
    "        \n",
    "        pred_from_primary = self.temporal_predictor(c_primary).unsqueeze(1).repeat(1, future_k, 1)\n",
    "        pred_from_secondary = self.temporal_predictor(c_secondary).unsqueeze(1).repeat(1, future_k, 1)\n",
    "        \n",
    "        p_primary = self.projector(c_primary)\n",
    "        p_secondary = self.projector(c_secondary)\n",
    "\n",
    "        tstcc_outputs = (\n",
    "            pred_from_primary, q_secondary[:, -future_k:, :],\n",
    "            pred_from_secondary, q_primary[:, -future_k:, :],\n",
    "            p_primary, p_secondary\n",
    "        )\n",
    "\n",
    "        if phase == 'phase1':\n",
    "            return tstcc_outputs\n",
    "\n",
    "        # --- MEMTO Path (for reconstruction and memory update in Phase 2) ---\n",
    "        # Using the primary view (e.g., weak augmentation) for MEMTO's core logic\n",
    "        q = q_primary\n",
    "        \n",
    "        # Query Update\n",
    "        attn_weights = F.softmax(torch.matmul(q, self.memory.T) / temperature, dim=-1)\n",
    "        retrieved_memory = torch.matmul(attn_weights, self.memory)\n",
    "\n",
    "        # Gated Memory Update (only during training)\n",
    "        if self.training:\n",
    "            attn_v = F.softmax(torch.matmul(self.memory, q.transpose(1, 2)) / temperature, dim=-1)\n",
    "            weighted_queries = torch.matmul(attn_v, q)\n",
    "            psi = torch.sigmoid(self.U_psi(self.memory) + self.W_psi(weighted_queries.mean(dim=0)))\n",
    "            self.memory.data = (1 - psi) * self.memory + psi * weighted_queries.mean(dim=0)\n",
    "\n",
    "        # Decoder\n",
    "        updated_queries = torch.cat([q, retrieved_memory], dim=-1)\n",
    "        reconstructed_x = self.decoder(updated_queries)\n",
    "\n",
    "        return reconstructed_x, attn_weights, tstcc_outputs\n",
    "\n",
    "# --- 3. Hybrid Agent for Training and Prediction ---\n",
    "class HybridAgent:\n",
    "    def __init__(self, input_dim, latent_dim=64, num_memory_items=10, lr=1e-4, future_k=5,\n",
    "                 l_memto_rec=1.0, l_memto_entr=0.01, l_tstcc_tc=1.0, l_tstcc_cc=0.7):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        self.model = MEMTO_TSTCC(\n",
    "            input_dim=input_dim, latent_dim=latent_dim, num_memory_items=num_memory_items\n",
    "        ).to(self.device)\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.num_memory_items = num_memory_items\n",
    "        self.future_k = future_k\n",
    "        self.lambdas = {'rec': l_memto_rec, 'entr': l_memto_entr, 'tc': l_tstcc_tc, 'cc': l_tstcc_cc}\n",
    "\n",
    "    # --- Augmentations ---\n",
    "    def _jitter(self, x, sigma=0.1): return x + torch.randn_like(x) * sigma\n",
    "    def _scale(self, x, sigma=0.1): return x * (torch.randn(x.shape[0], 1, 1, device=self.device) * sigma + 1.0)\n",
    "    def _permute(self, x, max_segments=5):\n",
    "        orig_steps = np.arange(x.shape[1]); num_segs = np.random.randint(1, max_segments)\n",
    "        ret = np.array_split(orig_steps, num_segs); np.random.shuffle(ret)\n",
    "        return x[:, np.concatenate(ret), :]\n",
    "    def _get_augmentations(self, x_batch):\n",
    "        x_weak = self._scale(self._jitter(x_batch))\n",
    "        x_strong = self._jitter(self._permute(x_batch))\n",
    "        return x_strong, x_weak\n",
    "\n",
    "    # --- Loss Calculators ---\n",
    "    def _calculate_tstcc_loss(self, tstcc_outputs, temp=0.2):\n",
    "        pred_s, z_w_f, pred_w, z_s_f, p_s, p_w = tstcc_outputs\n",
    "        \n",
    "        loss_tc = F.mse_loss(pred_s, z_w_f) + F.mse_loss(pred_w, z_s_f)\n",
    "        \n",
    "        p_s_norm = F.normalize(p_s, dim=1); p_w_norm = F.normalize(p_w, dim=1)\n",
    "        sim_matrix = torch.matmul(p_s_norm, p_w_norm.T)\n",
    "        \n",
    "        logits = sim_matrix / temp\n",
    "        labels = torch.arange(logits.shape[0], dtype=torch.long, device=self.device)\n",
    "        loss_cc = F.cross_entropy(logits, labels)\n",
    "        \n",
    "        return self.lambdas['tc'] * loss_tc + self.lambdas['cc'] * loss_cc\n",
    "\n",
    "    def _calculate_phase2_loss(self, x_orig, x_rec, attn_weights, tstcc_outputs):\n",
    "        # MEMTO losses\n",
    "        rec_loss = F.mse_loss(x_rec, x_orig)\n",
    "        entr_loss = -torch.sum(attn_weights * torch.log(attn_weights + 1e-12), dim=-1).mean()\n",
    "        memto_loss = self.lambdas['rec'] * rec_loss + self.lambdas['entr'] * entr_loss\n",
    "        \n",
    "        # TSTCC losses\n",
    "        tstcc_loss = self._calculate_tstcc_loss(tstcc_outputs)\n",
    "        \n",
    "        return memto_loss + tstcc_loss, memto_loss, tstcc_loss\n",
    "\n",
    "    def train(self, train_loader, phase1_epochs=10, phase2_epochs=40):\n",
    "        # --- Phase 1: TS-TCC Pre-training ---\n",
    "        print(\"--- Starting Phase 1: TS-TCC Representation Learning ---\")\n",
    "        self.model.train()\n",
    "        for epoch in range(phase1_epochs):\n",
    "            total_loss = 0\n",
    "            for i, (x_batch,) in enumerate(train_loader):\n",
    "                x_batch = x_batch.to(self.device)\n",
    "                x_strong, x_weak = self._get_augmentations(x_batch)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                tstcc_outputs = self.model(x_strong, x_weak, phase='phase1', future_k=self.future_k)\n",
    "                loss = self._calculate_tstcc_loss(tstcc_outputs)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            print(f\"Phase 1, Epoch {epoch+1}/{phase1_epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "        # --- Interlude: K-means Memory Initialization ---\n",
    "        print(\"\\n--- Initializing Memory with K-means ---\")\n",
    "        self.model.eval()\n",
    "        all_queries = []\n",
    "        with torch.no_grad():\n",
    "            for (x_batch,) in train_loader:\n",
    "                x_batch = x_batch.to(self.device)\n",
    "                queries = self.model.encoder(x_batch) # Use the trained encoder\n",
    "                all_queries.append(queries.cpu().numpy().reshape(-1, self.model.latent_dim))\n",
    "        \n",
    "        kmeans = KMeans(n_clusters=self.num_memory_items, random_state=0, n_init=10)\n",
    "        kmeans.fit(np.concatenate(all_queries, axis=0))\n",
    "        self.model.memory.data = torch.from_numpy(kmeans.cluster_centers_).float().to(self.device)\n",
    "        print(\"Memory initialized.\")\n",
    "\n",
    "        # --- Phase 2: Hybrid Training ---\n",
    "        print(\"\\n--- Starting Phase 2: Hybrid MEMTO + TS-TCC Training ---\")\n",
    "        self.model.train()\n",
    "        for epoch in range(phase2_epochs):\n",
    "            total_loss, total_memto, total_tstcc = 0, 0, 0\n",
    "            for i, (x_batch,) in enumerate(train_loader):\n",
    "                x_batch = x_batch.to(self.device)\n",
    "                x_strong, x_weak = self._get_augmentations(x_batch)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                x_rec, attn, tstcc_out = self.model(x_weak, x_strong, phase='phase2', future_k=self.future_k)\n",
    "                loss, memto_loss, tstcc_loss = self._calculate_phase2_loss(x_weak, x_rec, attn, tstcc_out)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                total_memto += memto_loss.item()\n",
    "                total_tstcc += tstcc_loss.item()\n",
    "            print(f\"Phase 2, Epoch {epoch+1}/{phase2_epochs}, Total Loss: {total_loss/len(train_loader):.4f} \"\n",
    "                  f\"(MEMTO: {total_memto/len(train_loader):.4f}, TS-TCC: {total_tstcc/len(train_loader):.4f})\")\n",
    "\n",
    "    def predict(self, test_loader):\n",
    "        self.model.eval()\n",
    "        anomaly_scores = []\n",
    "        with torch.no_grad():\n",
    "            for (x_batch,) in test_loader:\n",
    "                x_batch = x_batch.to(self.device)\n",
    "                \n",
    "                # Get reconstruction and queries for ISD and LSD calculation\n",
    "                q = self.model.encoder(x_batch)\n",
    "                x_rec, _, _ = self.model(x_batch, phase='predict')\n",
    "                \n",
    "                # ISD\n",
    "                isd = torch.pow(x_batch - x_rec, 2).sum(dim=-1)\n",
    "                \n",
    "                # LSD\n",
    "                dist = torch.cdist(q, self.model.memory.unsqueeze(0).repeat(q.shape[0], 1, 1))\n",
    "                lsd = torch.min(dist, dim=-1).values\n",
    "                \n",
    "                score = F.softmax(lsd, dim=-1) * isd\n",
    "                anomaly_scores.append(score.cpu().numpy())\n",
    "        \n",
    "        return np.concatenate(anomaly_scores, axis=0)\n",
    "\n",
    "\n",
    "# --- Example of how to use the agent ---\n",
    "if __name__ == '__main__':\n",
    "    BATCH_SIZE = 32\n",
    "    SEQ_LEN = 100\n",
    "    INPUT_DIM = 38\n",
    "    FUTURE_K = 10 \n",
    "\n",
    "    train_data = torch.randn(BATCH_SIZE * 10, SEQ_LEN, INPUT_DIM)\n",
    "    test_data = torch.randn(BATCH_SIZE * 2, SEQ_LEN, INPUT_DIM)\n",
    "\n",
    "    train_dataset = torch.utils.data.TensorDataset(train_data)\n",
    "    test_dataset = torch.utils.data.TensorDataset(test_data)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    agent = HybridAgent(\n",
    "        input_dim=INPUT_DIM,\n",
    "        latent_dim=64,\n",
    "        num_memory_items=20, # Increased for potentially richer representations\n",
    "        lr=1e-4,\n",
    "        future_k=FUTURE_K\n",
    "    )\n",
    "    \n",
    "    agent.train(train_loader, phase1_epochs=5, phase2_epochs=10) # Keep epochs low for demo\n",
    "    \n",
    "    scores = agent.predict(test_loader)\n",
    "    \n",
    "    print(f\"\\nTraining and prediction finished.\")\n",
    "    print(f\"Shape of anomaly scores: {scores.shape}\")\n",
    "    print(f\"Expected shape: ({len(test_data)}, {SEQ_LEN})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af784f6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearningEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
