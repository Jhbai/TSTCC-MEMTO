{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d028bc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Encoder Module ---\n",
    "class TSEncoder(nn.Module):\n",
    "    def __init__(self, input_dims, output_dims, hidden_dims=64, depth=3):\n",
    "        super(TSEncoder, self).__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.output_dims = output_dims\n",
    "        self.hidden_dims = hidden_dims\n",
    "        \n",
    "        layers = []\n",
    "        current_in = input_dims\n",
    "        for _ in range(depth):\n",
    "            layers.extend([\n",
    "                nn.Conv1d(current_in, hidden_dims, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm1d(hidden_dims),\n",
    "                nn.ReLU()\n",
    "            ])\n",
    "            current_in = hidden_dims\n",
    "        \n",
    "        layers.append(nn.Conv1d(hidden_dims, output_dims, kernel_size=1))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, input_dims)\n",
    "        x = x.permute(0, 2, 1) # to (batch, input_dims, seq_len)\n",
    "        out = self.network(x)\n",
    "        return out.permute(0, 2, 1) # to (batch, seq_len, output_dims)\n",
    "\n",
    "# --- 2. TS-TCC Core Model ---\n",
    "class TS_TCC(nn.Module):\n",
    "    def __init__(self, input_dims, latent_dims, temporal_unit_dims, n_heads=4, n_layers=2):\n",
    "        super(TS_TCC, self).__init__()\n",
    "        \n",
    "        self.encoder = TSEncoder(input_dims=input_dims, output_dims=latent_dims)\n",
    "        \n",
    "        # Temporal Contrasting Module\n",
    "        encoder_layers = nn.TransformerEncoderLayer(\n",
    "            d_model=latent_dims, \n",
    "            nhead=n_heads, \n",
    "            dim_feedforward=latent_dims*2,\n",
    "            dropout=0.1,\n",
    "            batch_first=True,\n",
    "            norm_first=True # Pre-norm as mentioned in the paper\n",
    "        )\n",
    "        self.temporal_transformer = nn.TransformerEncoder(encoder_layers, num_layers=n_layers)\n",
    "        \n",
    "        # This linear layer acts as Wk in the paper to predict future latent steps\n",
    "        self.temporal_predictor = nn.Linear(latent_dims, latent_dims)\n",
    "\n",
    "        # Contextual Contrasting Module (Projection Head)\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(latent_dims, latent_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(latent_dims, temporal_unit_dims)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_strong, x_weak, future_k=5):\n",
    "        # x_strong, x_weak: [batch, seq_len, input_dim]\n",
    "\n",
    "        # --- Get latent representations ---\n",
    "        z_strong = self.encoder(x_strong)\n",
    "        z_weak = self.encoder(x_weak)\n",
    "        \n",
    "        # --- Temporal Contrasting ---\n",
    "        # The past is all but the last k steps\n",
    "        z_strong_past = z_strong[:, :-future_k, :]\n",
    "        z_weak_past = z_weak[:, :-future_k, :]\n",
    "        \n",
    "        # The future is the last k steps\n",
    "        z_strong_future = z_strong[:, -future_k:, :]\n",
    "        z_weak_future = z_weak[:, -future_k:, :]\n",
    "        \n",
    "        # Get context vectors from the transformer\n",
    "        c_strong = self.temporal_transformer(z_strong_past).mean(dim=1) # Average pooling for context\n",
    "        c_weak = self.temporal_transformer(z_weak_past).mean(dim=1)\n",
    "\n",
    "        # Cross-view prediction\n",
    "        pred_from_strong = self.temporal_predictor(c_strong).unsqueeze(1).repeat(1, future_k, 1)\n",
    "        pred_from_weak = self.temporal_predictor(c_weak).unsqueeze(1).repeat(1, future_k, 1)\n",
    "        \n",
    "        # --- Contextual Contrasting ---\n",
    "        p_strong = self.projector(c_strong)\n",
    "        p_weak = self.projector(c_weak)\n",
    "        \n",
    "        return pred_from_strong, z_weak_future, pred_from_weak, z_strong_future, p_strong, p_weak\n",
    "\n",
    "\n",
    "# --- 3. Agent to handle training and representation extraction ---\n",
    "class TS_TCC_Agent:\n",
    "    def __init__(self, input_dims, latent_dims=64, temporal_unit_dims=128, \n",
    "                 lr=3e-4, lambda1=1.0, lambda2=0.7, temp=0.2, future_k=5):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        self.model = TS_TCC(\n",
    "            input_dims=input_dims,\n",
    "            latent_dims=latent_dims,\n",
    "            temporal_unit_dims=temporal_unit_dims\n",
    "        ).to(self.device)\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr, weight_decay=3e-4)\n",
    "        self.lambda1 = lambda1\n",
    "        self.lambda2 = lambda2\n",
    "        self.temp = temp\n",
    "        self.future_k = future_k\n",
    "    \n",
    "    # --- Augmentation Methods ---\n",
    "    def _jitter(self, x, sigma=0.1):\n",
    "        return x + torch.randn_like(x) * sigma\n",
    "\n",
    "    def _scale(self, x, sigma=0.1):\n",
    "        factor = torch.randn(x.shape[0], 1, 1, device=self.device) * sigma + 1.0\n",
    "        return x * factor\n",
    "\n",
    "    def _permute(self, x, max_segments=5, seg_mode=\"random\"):\n",
    "        orig_steps = np.arange(x.shape[1])\n",
    "        num_segs = np.random.randint(1, max_segments)\n",
    "        ret = np.array_split(orig_steps, num_segs)\n",
    "        np.random.shuffle(ret)\n",
    "        ret = np.concatenate(ret)\n",
    "        return x[:, ret, :]\n",
    "\n",
    "    def _get_augmentations(self, x_batch):\n",
    "        # Weak Augmentation\n",
    "        x_weak = self._scale(self._jitter(x_batch))\n",
    "        # Strong Augmentation\n",
    "        x_strong = self._jitter(self._permute(x_batch))\n",
    "        return x_strong, x_weak\n",
    "\n",
    "    # --- Loss Calculation ---\n",
    "    def _info_nce_loss(self, query, positive, negatives):\n",
    "        # query: [batch, D], positive: [batch, D], negatives: [batch, K, D]\n",
    "        query = F.normalize(query, dim=1)\n",
    "        positive = F.normalize(positive, dim=1)\n",
    "        negatives = F.normalize(negatives, dim=2)\n",
    "        \n",
    "        l_pos = (query * positive).sum(dim=1).unsqueeze(1) # [batch, 1]\n",
    "        l_neg = torch.bmm(query.unsqueeze(1), negatives.transpose(1, 2)).squeeze(1) # [batch, K]\n",
    "        \n",
    "        logits = torch.cat([l_pos, l_neg], dim=1)\n",
    "        labels = torch.zeros(logits.shape[0], dtype=torch.long, device=self.device)\n",
    "        return F.cross_entropy(logits / self.temp, labels)\n",
    "\n",
    "    def _calculate_loss(self, pred_from_strong, z_weak_future, pred_from_weak, z_strong_future, p_strong, p_weak):\n",
    "        batch_size = p_strong.shape[0]\n",
    "\n",
    "        # Temporal Contrastive Loss (L_TC)\n",
    "        loss_tc = F.mse_loss(pred_from_strong, z_weak_future) + F.mse_loss(pred_from_weak, z_strong_future)\n",
    "\n",
    "        # Contextual Contrastive Loss (L_CC)\n",
    "        p_strong_norm = F.normalize(p_strong, dim=1)\n",
    "        p_weak_norm = F.normalize(p_weak, dim=1)\n",
    "        \n",
    "        sim_matrix = torch.matmul(p_strong_norm, p_weak_norm.T) # [batch, batch]\n",
    "        \n",
    "        # Positive is the diagonal, negative is off-diagonal\n",
    "        positives = torch.diag(sim_matrix)\n",
    "        \n",
    "        # Create a mask to select negatives for each sample in the batch\n",
    "        mask = ~torch.eye(batch_size, dtype=torch.bool, device=self.device)\n",
    "        negatives = sim_matrix[mask].reshape(batch_size, batch_size - 1)\n",
    "        \n",
    "        logits = torch.cat([positives.unsqueeze(1), negatives], dim=1)\n",
    "        labels = torch.zeros(batch_size, dtype=torch.long, device=self.device)\n",
    "        loss_cc = F.cross_entropy(logits / self.temp, labels)\n",
    "        \n",
    "        total_loss = self.lambda1 * loss_tc + self.lambda2 * loss_cc\n",
    "        return total_loss, loss_tc, loss_cc\n",
    "\n",
    "    def train(self, train_loader, epochs=50):\n",
    "        self.model.train()\n",
    "        print(\"--- Starting TS-TCC Pre-training ---\")\n",
    "        for epoch in range(epochs):\n",
    "            total_loss, total_tc, total_cc = 0, 0, 0\n",
    "            for i, (x_batch,) in enumerate(train_loader):\n",
    "                x_batch = x_batch.to(self.device)\n",
    "                x_strong, x_weak = self._get_augmentations(x_batch)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                outputs = self.model(x_strong, x_weak, future_k=self.future_k)\n",
    "                loss, tc, cc = self._calculate_loss(*outputs)\n",
    "                \n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                total_tc += tc.item()\n",
    "                total_cc += cc.item()\n",
    "            \n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            avg_tc = total_tc / len(train_loader)\n",
    "            avg_cc = total_cc / len(train_loader)\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f} (TC: {avg_tc:.4f}, CC: {avg_cc:.4f})\")\n",
    "\n",
    "    def get_representations(self, data_loader):\n",
    "        self.model.eval()\n",
    "        all_reps = []\n",
    "        with torch.no_grad():\n",
    "            for (x_batch,) in data_loader:\n",
    "                x_batch = x_batch.to(self.device)\n",
    "                \n",
    "                # Get latent representation from the encoder\n",
    "                z = self.model.encoder(x_batch)\n",
    "                \n",
    "                # Get context vector from the temporal module\n",
    "                # The paper uses this context vector for downstream tasks\n",
    "                c = self.model.temporal_transformer(z).mean(dim=1)\n",
    "                \n",
    "                all_reps.append(c.cpu().numpy())\n",
    "        \n",
    "        return np.concatenate(all_reps, axis=0)\n",
    "\n",
    "\n",
    "# --- Example of how to use the agent ---\n",
    "if __name__ == '__main__':\n",
    "    # --- Dummy Parameters and Data for Demonstration ---\n",
    "    BATCH_SIZE = 32\n",
    "    SEQ_LEN = 128\n",
    "    INPUT_DIMS = 9\n",
    "    FUTURE_K = 10 # Predict last 10 steps\n",
    "\n",
    "    # Create dummy data\n",
    "    train_data = torch.randn(BATCH_SIZE * 10, SEQ_LEN, INPUT_DIMS)\n",
    "    train_dataset = torch.utils.data.TensorDataset(train_data)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    # Create the agent\n",
    "    agent = TS_TCC_Agent(\n",
    "        input_dims=INPUT_DIMS,\n",
    "        latent_dims=64,\n",
    "        temporal_unit_dims=128,\n",
    "        lr=3e-4,\n",
    "        lambda1=1.0,\n",
    "        lambda2=0.7,\n",
    "        temp=0.2,\n",
    "        future_k=FUTURE_K\n",
    "    )\n",
    "    \n",
    "    # Train the model (unsupervised pre-training)\n",
    "    agent.train(train_loader, epochs=5) # Keep low for demo\n",
    "    \n",
    "    # After training, extract representations for a downstream task (e.g., classification)\n",
    "    print(\"\\n--- Extracting representations for downstream task ---\")\n",
    "    representations = agent.get_representations(train_loader)\n",
    "    \n",
    "    print(f\"Shape of extracted representations: {representations.shape}\")\n",
    "    print(f\"Expected shape: ({len(train_data)}, temporal_unit_dims)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
